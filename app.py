
import os
import streamlit as st
import httpx # å¿…é¡»å¯¼å…¥è¿™ä¸ª
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate

# ==========================================
# ğŸ‘‡ 1. åŸºç¡€é…ç½®
# ==========================================
os.environ["http_proxy"] = ""
os.environ["https_proxy"] = ""
st.set_page_config(page_title="huilaiçš„æ™ºèƒ½ç ”æŠ¥åŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")

# ==========================================
# ğŸ‘‡ 2. å¡«å…¥ Key (è¯·æ£€æŸ¥ä½ çš„ Key æ˜¯å¦æ­£ç¡®ï¼)
# ==========================================
DEEPSEEK_API_KEY = "sk-1dd3d9aa14a14c8996afd6d9a74e2bad"
ZHIPUAI_API_KEY = "bfdb8628746c49849fb4eb767cfa9d07.RFuOavDnbMjGVnT9"
# ==========================================

# ä¸´æ—¶æ–‡ä»¶å¤¹
if not os.path.exists("./temp"):
    os.makedirs("./temp")

# ==========================================
# ğŸ‘‡ 3. æ ¸å¿ƒåŠŸèƒ½ï¼šå¤„ç†ä¸Šä¼ æ–‡ä»¶
# ==========================================
def process_uploaded_file(uploaded_file):
    file_path = os.path.join("./temp", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)
    
    embedding_model = ZhipuAIEmbeddings(model="embedding-2", api_key=ZHIPUAI_API_KEY)
    
    # å¼ºåˆ¶é‡æ–°åˆ›å»ºæ•°æ®åº“
    vector_db = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        persist_directory="./chroma_db"
    )
    return vector_db

# ==========================================
# ğŸ‘‡ 4. æ ¸å¿ƒåŠŸèƒ½ï¼šåŠ è½½é“¾ (Day 2 è®°å¿†ç‰ˆ)
# ==========================================
@st.cache_resource
def load_chain():
    # A. åŠ è½½ Embedding
    embedding_model = ZhipuAIEmbeddings(model="embedding-2", api_key=ZHIPUAI_API_KEY)
    
    # B. æ£€æŸ¥æ•°æ®åº“
    if not os.path.exists("./chroma_db"):
        return None
        
    db = Chroma(persist_directory="./chroma_db", embedding_function=embedding_model)
    
    # C. åŠ è½½å¤§æ¨¡å‹ (ä½æ¸©ä¸¥è°¨)
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=DEEPSEEK_API_KEY,
        base_url="https://api.deepseek.com",
        temperature=0.1
    )
    
    # D. å®šä¹‰äººè®¾ Prompt
    template = """
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„ AI ç ”æŠ¥åˆ†æåŠ©æ‰‹ï¼Œä½ çš„åå­—å«â€œä¼šæ¥â€ã€‚
    è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒæ–‡æ¡£ã€‘çš„å†…å®¹å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
    
    ã€å›ç­”è§„åˆ™ã€‘ï¼š
    1. è¯­æ°”è¦ä¸“ä¸šã€ä¸¥è°¨ã€‚
    2. ä¸¥ç¦çç¼–ï¼Œæ‰¾ä¸åˆ°ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚
    3. å›ç­”æœ€åï¼Œè¯·åŠ ä¸Šä¸€å¥ï¼šâ€œâ€”â€” ç”±ä¼šæ¥çš„ AI åŠ©æ‰‹ç”Ÿæˆâ€ã€‚

    ã€å‚è€ƒæ–‡æ¡£ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}

    è¯·å¼€å§‹åˆ†æå¹¶å›ç­”ï¼š
    """
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    # E. å®šä¹‰è®°å¿† Memory
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
    
    # F. ç»„è£…å¯¹è¯é“¾ (æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯ ConversationalRetrievalChain)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    
    return qa_chain

# ==========================================
# ğŸ‘‡ 5. ç•Œé¢ UI
# ==========================================
st.title("ğŸ¤– huilaiçš„æ™ºèƒ½ç ”æŠ¥åŠ©æ‰‹")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ“„ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"])
    
    if uploaded_file:
        if st.button("å¼€å§‹åˆ†æ"):
            with st.spinner("æ­£åœ¨æ‹†è§£æ–‡æ¡£ï¼Œè¯·ç¨ç­‰..."):
                try:
                    process_uploaded_file(uploaded_file)
                    st.success("âœ… åˆ†æå®Œæˆï¼æ•°æ®åº“å·²æ›´æ–°ã€‚")
                    # æ¸…ç©ºå†å²ï¼Œé˜²æ­¢ä¸²å°
                    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼æ–°æ–‡æ¡£å·²åŠ è½½ï¼Œè¯·é—®å§ï¼"}]
                    st.cache_resource.clear()
                    st.rerun()
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {e}")

# --- ä¸»ç•Œé¢èŠå¤© ---
chain = load_chain()

if not chain:
    st.warning("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ª PDF æ–‡æ¡£ï¼Œç‚¹å‡»â€œå¼€å§‹åˆ†æâ€ï¼")
else:
    # åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å·²ç»å‡†å¤‡å¥½å›ç­”å…³äºè¿™ä»½æ–‡æ¡£çš„é—®é¢˜äº†ã€‚"}]

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input():
        st.chat_message("user").write(prompt)
        st.session_state["messages"].append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨å›å¿†å¹¶æ€è€ƒ..."):
                try:
                    # ğŸ‘‡ å…³é”®ç‚¹ï¼šè¿™é‡Œå¿…é¡»ç”¨ "question"ï¼Œç»å¯¹ä¸èƒ½ç”¨ "query"
                    response = chain.invoke({"question": prompt})
                    
                    # ğŸ‘‡ å…³é”®ç‚¹ï¼šç»“æœåœ¨ "answer" é‡Œ
                    result = response["answer"]
                    
                    st.write(result)
                    st.session_state["messages"].append({"role": "assistant", "content": result})
                except Exception as e:
                    st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")